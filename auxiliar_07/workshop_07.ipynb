{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_07.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRu6GzVgbQP7"
      },
      "source": [
        "# Convolutional Neural Networks\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_07/feature_maps.png\" width=\"800\">\r\n",
        "\r\n",
        "Las Redes Neuronales Convolucionales (CNN) consisten en arquitecturas muy similares a las redes `Dense` o Fully Connected vistas en el workshop anterior, pues estás también se constituyen de de nodos con parámetros entrenables, como son los pesos y biases encargados de ponderar la información de entrada o `input.`\r\n",
        "\r\n",
        "No obstante, la estructura de las Redes Convolucionales está diseñada particularmente para interpretar imágenes y aprender, durante su entrenamiento, a extraer patrones y características espaciales (`features`) de estas mediante la aplicación secuencial de filtros. En este sentido, este tipo de modelos han probado desempeñarse bastante bien en tareas de visión computacional, tales como reconocimiento y clasificación de objetos en imágenes, siendo esta eficacia una de las principales razones del reconocimiento del potencial del Deep Learning en las últimas décadas.\r\n",
        "\r\n",
        "![animation](https://miro.medium.com/max/1200/1*QPRC1lcfYxcWWPAC2hrQgg.gif \"animation\")\r\n",
        "\r\n",
        "A diferencia de las capas `Dense`, las capas `Convolucionales` en realidad se componen de filtros similares a los vistos en el primer capitulo de visión computacional del curso. De este modo, los pesos de la red pasan a ser estructurados como los parámetros que definen el `kernel` del filtro. Durante el procesamiento de la imágen al interior de la red, los filtros de cada capa van generando nuevas imágenes (`feature maps`), con la finalidad de ir segmentando las `features` principales que permiten aislar la información importante de los datos.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_07/filtro_diagram.png\" height=\"200\">\r\n",
        "\r\n",
        "Con la finalidad de introducir la implementación de este tipo de arquitecturas en `Tensorflow` estudiaremos un caso de regresión simple, en donde entrenaremos un modelo CNN para detectar la posición $(x, z)$ de una esfera en un espacio tridimensional.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/tensorflow.png\" width=\"400\">\r\n",
        "\r\n",
        "`TensorFlow`, en términos generales, consiste en un framework diseñado para desarrollar e implementar algoritmos de Machine Learning, y por supuesto, entre ellos, modelos de Deep Learning. Una de las particulares de este framework es que ofrece toda una gama de niveles de abstracción, desde el desarrollo de modelos de mayor complejidad mediante herramientas `low-level`, hasta la compilación y el entrenamiento de arquitecturas mediante estructuras `high-level`, como la API Keras.\r\n",
        "\r\n",
        "Puede encontrar la documentación de estas librerías en los siguientes links:\r\n",
        "- https://www.tensorflow.org/api_docs/python/\r\n",
        "- https://keras.io/api/\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIgVTEjm3hVG"
      },
      "source": [
        "!git clone https://github.com/cherrerab/roboticafcfm.git\r\n",
        "%cd /content/roboticafcfm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF1NTCH2vuNP"
      },
      "source": [
        "## Ray Tracing Dataset\r\n",
        "\r\n",
        "El Ray Tracing consiste en un algoritmo de renderizado de imágenes que desempeña la tarea de calcular el camino de la luz como píxeles en un plano de la imagen y simular sus efectos sobre las superficies virtuales en las que incide. En este caso, para generar nuestro dataset de esferas en un espacio 3D, utilizaremos el algoritmo desarrollado por James Bowman. Este algoritmo disponible en el github `https://github.com/mdoege/raytrace` nos permite renderizar esferas parametrizables en un espacio tridimensional, como se muestra en la imagen a continuación.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_07/spheres_examples.png\" height=\"200\">\r\n",
        "\r\n",
        "De este modo, utilizaremos este programa para generar una serie de imágenes (`samples`) de `128x128px` que contengan una esfera en distintas posiciones en el espacio. Para ahorrar tiempo y concentrarnos en el desarrollo del modelo, este proceso ya ha sido realizado y el dataset `CNN_dataset_128px.npz` correspondiente ha sido cargado a un Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0UAuLiFbBBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144c4c1a-2267-45a0-e0e4-8b74961e3e86"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "\r\n",
        "import os\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "# inicializar GoogleDrive con credenciales de autorización\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)\r\n",
        "\r\n",
        "# crear carpeta para descargar los archivos .npz\r\n",
        "!mkdir /content/datasets\r\n",
        "\r\n",
        "# Google Drive IDs para descargar los archivos .npz\r\n",
        "files_id = [('CNN_dataset_128px.npz', '1nDDvUBunpRJpTzGaQgdeRVmBTbCxa3bJ')]\r\n",
        "\r\n",
        "# comenzar descarga\r\n",
        "print('descargando datasets: ', end='')\r\n",
        "\r\n",
        "for filename, id in files_id:\r\n",
        "  save_path = os.path.join('/content/datasets', filename)\r\n",
        "\r\n",
        "  # descargar y guardar en /content/datasets\r\n",
        "  downloaded = drive.CreateFile({'id': id}) \r\n",
        "  downloaded.GetContentFile(save_path)\r\n",
        "\r\n",
        "# indicar descarga terminada\r\n",
        "print('done')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/datasets’: File exists\n",
            "descargando datasets: done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMC8t7t1zNvc"
      },
      "source": [
        "Podemos cargar este archivo mediante `np.load()` y explorar las estructuras y datos que contiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMQPOFQozG30"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# ---\r\n",
        "# cargar archivo CNN_dataset_128px.npz\r\n",
        "dataset = np.load('/content/datasets/CNN_dataset_128px.npz', allow_pickle=True)\r\n",
        "\r\n",
        "# print keys del dataset\r\n",
        "print('dataset.keys: ',  list( dataset.keys() ) )\r\n",
        "\r\n",
        "# ---\r\n",
        "# extraer conjuntos de imágenes y normalizar en [0., 1.]\r\n",
        "X = \r\n",
        "X =\r\n",
        "\r\n",
        "# extraer conjunto de posiciones (x, z)\r\n",
        "Y = \r\n",
        "\r\n",
        "# ---\r\n",
        "# visualizar muestra del dataset\r\n",
        "sample_idx = np.random.choice( np.arange(X.shape[0]), 5 )\r\n",
        "img_sample = [ X[i, :, :, :].reshape( (128, 128, 3) ) for i in sample_idx ]\r\n",
        "img_sample = np.hstack(img_sample)\r\n",
        "\r\n",
        "plt.figure( figsize=(12, 12) )\r\n",
        "plt.imshow(img_sample)\r\n",
        "\r\n",
        "print('Y:\\n', Y[sample_idx, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ_lR7X_0Iob"
      },
      "source": [
        "---\r\n",
        "## Data Splitting\r\n",
        "\r\n",
        "Análogamente al caso estudiado en el workshop anterior, teniendo ya el dataset para el entrenamiento de la red neuronal, se debe dividir este en dos sets: uno de entrenamiento (`training set`) y otro de testing (`testing set`). El primero es utilizado, como su nombre lo indica, en el entrenamiento de la red neuronal, mientras que el segundo es utilizado para evaluar el desempeño del modelo ya entrenado.\r\n",
        "\r\n",
        "El `data splitting` se puede lograr con el bloque de código a continuación, mediante la función `train_test_split` de `sklearn`.\r\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa43Krjc0lkj"
      },
      "source": [
        "# importar librerías\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# ---\r\n",
        "# generar sets de datos de training y testing\r\n",
        "# la varibale test_size permite controlar la proporción entre los datos de testing y training.\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = )\r\n",
        "\r\n",
        "# adicionalmente generaremos un conjunto de validación\r\n",
        "# este conjunto será utilizado para monitorear la generalización del modelo\r\n",
        "# durante el entrenamiento, sin utilizar el conjunto de testing.\r\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = )\r\n",
        "\r\n",
        "# print sample distribution\r\n",
        "print( 'train split: {:d} samples'.format(X_train.shape[0]) )\r\n",
        "print( '\\nvalidation split: {:d} samples'.format(X_val.shape[0]) )\r\n",
        "print( '\\ntesting split: {:d} samples'.format(X_test.shape[0]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO96UZZf1lMb"
      },
      "source": [
        "---\r\n",
        "# Model Building\r\n",
        "\r\n",
        "Para configurar nuestro modelo de regresión utilizaremos nuevamente la librería `keras` o `tf.keras`. Como ya introducimos en el workshop anterior, Keras es una API de alto nivel para la creación y el entrenamiento de modelos de deep learning. Está orientada y diseñada para la construcción de modelos de forma modular o en bloques. De este modo, ofrece un framework mucho más amigable e intuitivo para principiantes, a la vez que mantiene un estructura personalizable y versátil que permite a usuarios más avanzados incorporar nuevas ideas.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/keras_logo.png\" width=\"400\">\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukR_M6UE10OD"
      },
      "source": [
        "## Model Setup\r\n",
        "\r\n",
        "Dado que en este caso nuestros datos o `samples` consisten en imágenes a color en RGB, implementaremos una red neuronal convolucional de regresión (CNN) para procesar las imágenes sin perder la bidimensionalidad de su información.\r\n",
        "\r\n",
        "Por lo general, los modelos CNN se componen de series de capas `keras.layers.Conv2D` junto con algún tipo de Pooling Layer, como las `keras.layers.MaxPool2D` o las `keras.layers.AveragePooling2D`. \r\n",
        "\r\n",
        "- https://keras.io/api/layers/convolution_layers/convolution2d/\r\n",
        "- https://keras.io/api/layers/pooling_layers/\r\n",
        "\r\n",
        "El propósito de las Pooling Layers es realizar un `down-sampling` de los `feature maps` generados por las capas `Conv2D` y de este modo, reducir significativamente la dimensionalidad de la información a medida que esta avanza en el modelo y alcanza mayores grados de abstracción.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_07/CNN_diagram.png\" height=\"200\">\r\n",
        "\r\n",
        "Finalmente, para completar el modelo es necesario unir la información de los `feature maps` con la capa de salida `linear` que entregará las predicciones de la posición de las esferas. Para llevar esto a cabo, el tensor de los `feature maps` es vectorizado o aplanado en una sola dimensión mediante un `keras.layers.Flatten` para luego continuar con una serie de capas `keras.layers.Dense` que se encargan de terminar el procesamiento de la información."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCTB93K32qUz"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "from keras.layers import AveragePooling2D\r\n",
        "\r\n",
        "# inicializar modelo keras.Sequential\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# ---\r\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\r\n",
        "# las dimensiones de los datos que se ingresarán al modelo\r\n",
        "# las capas Conv2D reciben tensores de la forma (height, width, channels)\r\n",
        "input_dim = ( 128, 128, 3)\r\n",
        "model.add( Input( shape=input_dim ) )\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora debemos ir agregando nuestras capas Conv2D y Pooling.\r\n",
        "\r\n",
        "# las keras.layers.Conv2D reciben la cantidad de filtros dentro de la capa,\r\n",
        "# el tamaño de estos filtros y la función de activación con que operarán.\r\n",
        "# https://keras.io/api/layers/convolution_layers/convolution2d/\r\n",
        "\r\n",
        "# las keras.layers.MaxPooling2D reciben el tamaño de la ventana sobre\r\n",
        "# la cual llevarán a cabo el down-sampling\r\n",
        "# https://keras.io/api/layers/pooling_layers/max_pooling2d/\r\n",
        "\r\n",
        "model.add( Conv2D(32, (5, 5), activation='relu', padding='same' ) )\r\n",
        "model.add( Conv2D(32, (5, 5), activation='relu', padding='same' ) )\r\n",
        "\r\n",
        "model.add( MaxPooling2D( pool_size=(2, 2) ) )\r\n",
        "\r\n",
        "model.add( )\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora debemos ir agregando nuestras capas Dense para procesar la\r\n",
        "# información hasta la capa de salida.\r\n",
        "# https://keras.io/api/layers/core_layers/dense/\r\n",
        "\r\n",
        "model.add( Flatten() )\r\n",
        "\r\n",
        "model.add( Dense(units=256, activation='relu') )\r\n",
        "model.add( )\r\n",
        "\r\n",
        "# ---\r\n",
        "# por último debemos configurar nuestra capa de salida\r\n",
        "# dado que el modelo consiste en uno de regresión emplearemos\r\n",
        "# la función linear, donde cada nodo indicará la predicción de posición\r\n",
        "# de la esfera correspondiente.\r\n",
        "labels_num = \r\n",
        "model.add( Dense(units=labels_num, activation='linear') )\r\n",
        "\r\n",
        "# print model.summary()\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jjAUHaq3Lsg"
      },
      "source": [
        "## Compile Model\r\n",
        "\r\n",
        "Antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\r\n",
        "- https://keras.io/api/models/model_training_apis/\r\n",
        "- https://keras.io/api/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebFJrNsx3OmA"
      },
      "source": [
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "# configurar optimizador Adam\r\n",
        "# https://keras.io/api/optimizers/adam/\r\n",
        "opt = Adam( learning_rate=1e-3 )\r\n",
        "\r\n",
        "# ---\r\n",
        "# compilar modelo siguiendo como función de pérdida\r\n",
        "# el error cuadrado medio\r\n",
        "model.compile(loss= , optimizer=opt, metrics=[ ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC0A5nTJ3Tso"
      },
      "source": [
        "## Model Training\r\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros.\r\n",
        "\r\n",
        "Para entrenar `keras.Models` se utiliza el método `keras.Model.fit`, el cual aparte de iniciar y realizar la rutina de entrenamiento, retorna un registro `History`. Mediante `History.history` es posible acceder a la evolución de la función de pérdida durante el entrenamiento tanto sobre los datos de `train` como sobre los de `validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xPNb8sk3Y5_"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from utils import plot_loss_function\r\n",
        "\r\n",
        "# realizar rutina de entrenamiento\r\n",
        "model_history = model.fit(X_train, Y_train,\r\n",
        "                          batch_size= , epochs= ,\r\n",
        "                          validation_data=(X_val, Y_val))\r\n",
        "\r\n",
        "# plot gráfico de función de pérdida\r\n",
        "plot_loss_function(model_history, figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwRV_vU73Ypn"
      },
      "source": [
        "## Model Evaluation\r\n",
        "Finalmente, una vez entrenado nuestro modelo debemos evaluar su desempeño. En este caso particular, debemos usar los datos que aislamos para `testing` `(X_test, Y_test)`. Para utilizar el `keras.Model` sobre nuevos datos de clasificación, conviene utilizar el método `keras.Sequential.predict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5NdBs783yoY"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "\r\n",
        "from utils import plot_classification_map\r\n",
        "from utils import plot_confusion_matrix\r\n",
        "\r\n",
        "# obtener predicciones de X_test con model.predict\r\n",
        "Y_pred = \r\n",
        "Y_true = \r\n",
        "\r\n",
        "# calcular error mse de predicción\r\n",
        "mse = mean_squared_error(Y_true, Y_pred)\r\n",
        "print('testing mse: {:2.3f}'.format(mse))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUfhOU1L4sTg"
      },
      "source": [
        "Otra forma de evaluar nuestro modelo es utilizar el mismo algoritmo de `Ray Tracing` utilizado para generar el dataset. Así, utilizaremos las predicciones de posición retornadas por el modelo CNN para renderizar una nueva imagen y comprobar si esta se corresponde con la ingresada al modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUF0lBQW5N-u"
      },
      "source": [
        "# cargar GitHub https://github.com/mdoege/raytrace.git\r\n",
        "%cd /content/\r\n",
        "!git clone https://github.com/mdoege/raytrace.git\r\n",
        "%cd /content/raytrace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HodWr7fG5RhB"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2\r\n",
        "\r\n",
        "from rt6 import *\r\n",
        "\r\n",
        "# obtener sample de testing\r\n",
        "sample_idx = np.random.randint( X_test.shape[0] )\r\n",
        "\r\n",
        "# obtener predicciones de posición mediante model.predict\r\n",
        "img_test = X_test[sample_idx, :, :, :].reshape(1, 128, 128, 3)\r\n",
        "Y_pred = model.predict( img_test )\r\n",
        "\r\n",
        "print('Y_true: ', Y_test[sample_idx, :])\r\n",
        "print('Y_pred: ', Y_pred)\r\n",
        "\r\n",
        "# asignar predicciones a valores (x, z)\r\n",
        "posx, posz = Y_pred[0, :]\r\n",
        "\r\n",
        "# renderizar imagen a partir de (posx, posz)\r\n",
        "scene = [\r\n",
        "        Sphere(vec3(posx, .1, posz), .6, rgb(1.0, 0.0, 0.0)),\r\n",
        "        CheckeredSphere(vec3(0,-99999.5, 0), 99999, rgb(.75, .75, .75), 0.25),\r\n",
        "        ]\r\n",
        "render(scene, '/content/render.png')\r\n",
        "\r\n",
        "# ---\r\n",
        "# procesar y visualizar imagen\r\n",
        "img_pred = cv2.imread('/content/render.png')\r\n",
        "img_pred = cv2.resize(img_pred[:, 420:1500,:], dsize=(128, 128))\r\n",
        "\r\n",
        "# comparar a imagen original\r\n",
        "img_test = np.reshape( img_test, (128, 128, 3))\r\n",
        "img = np.hstack([img_test, img_pred/255.0])\r\n",
        "\r\n",
        "plt.figure( figsize=(8, 8) )\r\n",
        "plt.imshow( img )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVku7HQcIaEt"
      },
      "source": [
        "%cd /content/roboticafcfm\n",
        "from keras.models import Model\n",
        "from utils import plot_img_samples\n",
        "\n",
        "for j in range(11):\n",
        "  # ---\n",
        "  # compilar submodelo de la CNN\n",
        "  input = model.input\n",
        "  CNN_output = model.layers[j].output\n",
        "  fmap_model = Model(input, CNN_output)\n",
        "\n",
        "  # obtener dimensiones de los feature maps\n",
        "  n_filters = CNN_output.shape[3]\n",
        "  height, width = CNN_output.shape[1:3]\n",
        "\n",
        "  # ---\n",
        "  # extraer CNN feature maps\n",
        "  img = X_train[50, :, :, :]\n",
        "  x = np.reshape(img, (1, 128, 128, 3))\n",
        "  CNN_fmap = fmap_model.predict(x)\n",
        "\n",
        "  # reordenar feature maps a (feature_maps, height, width)\n",
        "  fmaps = np.zeros( (n_filters, height, width) )\n",
        "  for i in range( n_filters ):\n",
        "    fmap = CNN_fmap[:, :, :, i]\n",
        "    fmaps[i, :, :] = np.reshape( fmap, (1, height, width) )\n",
        "\n",
        "  # visulizar fmaps mediante plot_img_samples\n",
        "  plot_img_samples(fmaps, range(30), grid=(3, 10),\n",
        "                  figsize=(15,15),title='CNN feature maps: ' + CNN_output.name)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}