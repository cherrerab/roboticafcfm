{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_03",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLmYUqDK-lDN"
      },
      "source": [
        "# SIFT Feature Detection\n",
        "\n",
        "El Scale-Invariant Feature Transform (SIFT) es un algoritmo, desarrollado por D. Lowe en 2004, para la extracción de `features` de una imagen. En un sentido abstracto, las `features` de una imagen son patrones o regiones fácilmente detectables y trackeables, a partir de las cuales podemos identificar y reconocer toda de objetos presentes en la imagen. Así, en términos simples, el algoritmo SIFT detecta puntos de interés `keypoints` dentro de una imágen y luego, si estos son lo suficientemente distinguibles, genera un desciptor para cada `keypoint` que permita identificarlo o, más importante, compararlo en el futuro.\n",
        "\n",
        "https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n",
        "\n",
        "Análogamente a los workshops que hemos trabajado en las auxiliares pasadas, utilizaremos la implementación de SIFT en OpenCV para identificar cartas Pokemon en una foto, a partir de una serie de cartas de referencia. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_03/bin/banner.png\" height=\"240\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N74OyWRoE193"
      },
      "source": [
        "## OpenCV\n",
        "\n",
        "La Open Surce Computer Vision es una librería especializada en herramientas de visión computacional y en todo lo que respecta a visión artificial en general. De este modo provee de funcionalidades de todo tipo de complejidad, desde operaciones básicas de procesamiento de imágenes, hasta algoritmos de reconocimiento de objetos.\n",
        "\n",
        "Por supuesto, esta librería cuenta con su documentación correspondiente. En esta puede encontrar descripciones más detalladas de sus funcionalidades, así como también ejemplos, tutoriales y otros.\n",
        "\n",
        "https://docs.opencv.org/master/\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_01/bin/opencv_logo.png\" height=\"200\">\n",
        "\n",
        "Ahora, debido a conflictos de patentes la implementación de SIFT en OpenCV tuvo que ser movida a una librería complementaria `opencv-contrib`. De esta forma, para utilizarla en el workshop debemos instalar la librería en el entorno de Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxSUg_dXJUMg"
      },
      "source": [
        "!pip install opencv-python==3.4.2.16 opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkz_N655GS18"
      },
      "source": [
        "---\n",
        "## Load Reference Images\n",
        "\n",
        "La imagen que contiene la carta Pokemon que queremos identificar se encuentra en el archivo `img_01.jpg` dentro del github del curso. No obstante, antes de procesar ésta debemos extraer las `features` de una serie de cartas de referencia para así tener los `keypoints` y `descriptors` con los cuales podremos posteriormente comparar nuestra imagen de interés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ6Fjw79IFLR"
      },
      "source": [
        "!git clone https://github.com/cherrerab/roboticafcfm.git\n",
        "%cd /content/roboticafcfm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZnJENiMILH2"
      },
      "source": [
        "A continuación cargaremos y exploraremos todas las imagenes contenidas en la carpeta `auxiliar_03/pokemon_cards`.\n",
        "\n",
        "Podemos ver con cuantas cartas de referencia contamos mediante la función `os.listdir`, la cual retornará un listado de todos los archivos contenidos en la carpeta. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vluGJla4M2Zm"
      },
      "source": [
        "import os\n",
        "\n",
        "# obtener lista con todos los archivos contenidos en pokemon_cards\n",
        "# para esto podemos usar la función os.listdir\n",
        "dir_path = 'auxiliar_03//pokemon_cards'\n",
        "img_list = \n",
        "\n",
        "# print lista de cartas\n",
        "print('cartas de referencia:\\n', img_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKpjbIw1NWxQ"
      },
      "source": [
        "Así, en `auxiliar_03/pokemon_cards` hay 6 archivos `.jpg`, donde cada uno de estas imágenes se corresponde con una carta pokemon. Ahora, recorreremos esta lista para cargar las imágenes utilizando `cv2.imread` y ajustarlas a un tamaño de `350x250px` mediante `cv2.resize`. De esta manera, generaremos una lista `ref_imgs` que contendrá los `np.array` con las imágenes de referencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUZBahPJs9V"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "\n",
        "# inicializar lista que contendrá las imágenes np.array,\n",
        "# también una que contendrá el nombre de los pokemones\n",
        "ref_imgs, ref_names = [], []\n",
        "\n",
        "# por cada imagen .jpg en img_list\n",
        "for filename in img_list:\n",
        "  # obtener nombre utilizando os.path.splitext\n",
        "  img_name, _ = os.path.splitext(filename)\n",
        "\n",
        "  # generar path del archivo con os.path.join\n",
        "  img_path = os.path.join(dir_path, filename)\n",
        "\n",
        "  # load imagen con cv2.imread\n",
        "  img = \n",
        "\n",
        "  # resize a tamaño de 350x250px con cv2.resize\n",
        "  img = \n",
        "\n",
        "  # agregar a listas\n",
        "  ref_imgs.append(  )\n",
        "  ref_names.append(  )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# concatenar cartas de referencia mediante np.hstack\n",
        "cards = np.hstack( ref_imgs )\n",
        "\n",
        "# visualizar\n",
        "cards = cv2.cvtColor(cards, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure( figsize=(15,15) )\n",
        "plt.imshow(cards)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc9oCh4DPPuW"
      },
      "source": [
        "Excelente, hemos logrado generar la lista `ref_images` con todas nuestras cartas de referencia. Esta lista nos permitirá recorrer y procesar las imágenes de manera más cómoda y elegante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioex-5yFP1KO"
      },
      "source": [
        "## Extract Reference KeyPoints\n",
        "\n",
        "Ahora procesaremos esta serie de cartas con SIFT para extraer sus `KeyPoints` y generar sus `Descriptors` correspondientes. La implementación de SIFT se encuentra en el módulo `cv2.xfeatures2d` el cual contiene distintos métodos de extracción de features y detección de `keypoints` como SIFT y SURF.\n",
        "\n",
        "https://docs.opencv.org/3.4.2/d0/d13/classcv_1_1Feature2D.html\n",
        "\n",
        "Para utilizar SIFT primero hay que inicializar el objeto mediante la función `cv2.xfeatures2d.SIFT_create` y luego utilizar la función `cv2.xfeatures2d_SIFT.detectAndCompute` sobre la imágen de interés para detectar los `Keypoints` y generar los `Descriptors`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb-2PjNMQwXv"
      },
      "source": [
        "# inicializar SIFT detector (cv2.xfeatures2d_SIFT)\n",
        "sift = \n",
        "\n",
        "# incializar lista que contendrá los keypoints y descriptores\n",
        "# de las cartas de referencia\n",
        "ref_features = \n",
        "\n",
        "# para cada una de las cartas en ref_imgs\n",
        "for img in ref_imgs:\n",
        "\n",
        "  # transformar a escala de grises\n",
        "  gray = \n",
        "\n",
        "  # computar keypoint y descritores con sift.detectAndCompute\n",
        "  kps, des = \n",
        "\n",
        "  # agregar a lista de features, (kps, des)\n",
        "  ref_features.append(  )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CrpIUtmTl3p"
      },
      "source": [
        "Una vez generadas las `features` de una imagen, estas pueden ser visualizadas mediante la función `cv2.drawKeypoints`. Si a esta función se le entrega como parámetro `flags` el objeto `cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS`, dibujará los `KeyPoints` utilizando la escala y orientación de estos.\n",
        "\n",
        "https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html\n",
        "\n",
        "Como ejemplo, a continuación dibujaremos los `KeyPoints` para la carta `Meowth`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gour0_oUqNK"
      },
      "source": [
        "# index en ref_imgs correspondiente a la carta meowth\n",
        "idx = 1\n",
        "\n",
        "# extraer imagen y convertir a escala de grises\n",
        "gray = \n",
        "\n",
        "# extraer keypoints y descriptores\n",
        "kps, des = \n",
        "\n",
        "# generar imagen donde se dibujarán los keypoints\n",
        "out = np.zeros_like(gray)\n",
        "\n",
        "# dibujar keypoints, cv2.drawKeypoints\n",
        "out = \n",
        "\n",
        "# visualizar\n",
        "plt.figure( figsize=(10,10) )\n",
        "plt.imshow(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_IWtuYHWm41"
      },
      "source": [
        "---\n",
        "## Feature Matching\n",
        "\n",
        "Ya que tenemos las `features` de nuestras cartas de referencia en la lista `ref_features`, podemos pasar ahora a procesar nuestra imágen de interés `img_01.jpg`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGlHZDunbn6q"
      },
      "source": [
        "# cargar imagen img_01.jpg con cv2.imread\n",
        "img_path = 'auxiliar_03//img_01.jpg'\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# convertir a RGB para visualizar\n",
        "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# visualizar\n",
        "plt.figure( figsize=(10,10) )\n",
        "plt.imshow(rgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juf0W9BEcKzI"
      },
      "source": [
        "Del mismo modo en el que procesamos las imágenes de referencia, extraeremos los `KeyPoints` y `Descriptors` de esta imagen utilizando el mismo objeto `cv2.xfeatures2d_SIFT` anteriormente inicializado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WHMkYZconr"
      },
      "source": [
        "# para reducir el costo computacional reducir tamaño a (350x525)\n",
        "# para transformar el tamaño se puede utilizar cv2.resize\n",
        "img = cv2.resize(   )\n",
        "\n",
        "# convertir imagen a escala de grises\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# computar keypoint y descritores\n",
        "kps, des = \n",
        "\n",
        "# inicializar imagen para visualizar\n",
        "out = np.zeros_like(gray)\n",
        "\n",
        "# dibujar keypoints\n",
        "out = cv2.drawKeypoints(gray, kps, out, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# visualizar\n",
        "plt.figure( figsize=(10,10) )\n",
        "plt.imshow(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7wwppVsflxS"
      },
      "source": [
        "Ahora que tenemos las `features` de nuestra imagen de interés debemos, debemos realizar un `matching` respecto a las `features` de las cartas de referencia para analizar la cantidad de coincidencias y así, qué carta posee la mayor cantidad de coincidencias.\n",
        "\n",
        "Para esto, dado que los descriptores consisten en vectores de `(128, )` o bien, de 128 dimensiones, podemos utilizar como criterio la distancia euclidiana para evaluar que tan similares son dos `Descriptors` entre si o que tan cerca se encuentran en este espacio de 128 dimensiones. Así, realizaremos un `matching` de fuerza bruta mediante un `cv2.BFMatcher`, el cual a partir de un set de descriptores de `query` buscará los descriptores más cercanos en un set de `reference` para cada descriptor en el primer set.\n",
        "\n",
        "https://docs.opencv.org/3.4/d3/da1/classcv_1_1BFMatcher.html\n",
        "\n",
        "No obstante, cómo podemos validar si los `matches` realizados por `cv2.BFMatcher` son buenas coincidencias, pues siempre se podrá encontrar el descriptor más cercano a otro, incluso si estos no se encuentran a poca distancia.\n",
        "\n",
        "Para resolver esto D. Lowe propone utilizar un `Ratio Test` para validar los `matches`. Primero, en vez de buscar el descriptor más cercano a otro, se buscan los 2 descriptores más cercanos. Luego, bajo el supuesto de que los descriptores generados por SIFT son lo suficientemente distintivos, no debiese ser posible que hayan dos coincidencias o `matches`. Así, si existe un `match` válido, la distancia de la primera coincidencia debería ser considerablemente menor a la de la segunda. Dicho de otra forma, si ambas distancias de `match` son similares, entonces la verdad es que no se hayó ninguna coincidencia significativa.\n",
        "\n",
        "Para obtener los `k` descriptores más cercanos, podemos usar el método `cv2.BFMatcher.knnMatch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXTvjyfKkvo4"
      },
      "source": [
        "# inicializar BruteForce Matcher\n",
        "# con criterio de distancia euclidiana, cv2.NORM_L2\n",
        "BruteForce = \n",
        "\n",
        "# inicilizar lista que contendrá la cantida de coincidencias\n",
        "ref_votes = np.zeros( (6, ) )\n",
        "\n",
        "# para cada carta de referencia\n",
        "for i, feature in enumerate( ref_features ):\n",
        "  # extraer keypoints y descriptores\n",
        "  kps_ref, des_ref = \n",
        "\n",
        "  # para cada decriptor de img_01.jpg, encontrar los 2 mejores matches\n",
        "  # con respecto a los descriptores de las cartas de referencia\n",
        "  matches = \n",
        "\n",
        "  # por cada par de matches (match_1, match_2) obtenido\n",
        "  # para cada descriptor en des_ref\n",
        "  for match_1, match_2 in matches:\n",
        "\n",
        "    # aplicar ratio test para filtrar matches\n",
        "    ratio = \n",
        "    if match_1.distance < ratio*match_2.distance:\n",
        "      # agregar voto de match válido\n",
        "      ref_votes[i] += 1\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# print cantidad de votos\n",
        "print('cantidad de votos:\\n', ref_votes)\n",
        "\n",
        "# obtener carta de coincidencia y probabilidad\n",
        "ref_idx = \n",
        "proba = \n",
        "\n",
        "# print resultado\n",
        "name = ref_names[ref_idx]\n",
        "print('\\n{:10s} {:2.1f}%'.format(name, proba*100))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}