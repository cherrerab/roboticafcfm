{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8ogZOOz8xMD"
      },
      "source": [
        "# Deep Learning\r\n",
        "\r\n",
        "En el workshop anterior trabajamos sobre el campo del Machine Learning de Reinforcement Learning, donde los modelos son desarrollados y entrenados para evaluar y tomar decisiones en entornos dinámicos. En este caso, el entrenamiento es llevado a cabo mediante _recompenzas_ ante aciertos y _castigos_ ante errores, no obstante esta no es la única forma de realizar el entrenamiento sobre un modelo de redes neuronales.\r\n",
        "\r\n",
        "En el campo del Aprendizaje Supervizado, los modelos son entrenados para inferir y _aprender_ las _features_ y los patrones contenidos en una colección de datos para desempeñar alguna tarea de clasificación y/o regresión. En este caso, para entrenar los modelos es necesario contar con una serie de datos de entrada `X` con los resultados deseados de estos `Y`, también llamados datos de salida.\r\n",
        "\r\n",
        "De este modo, a diferencia del Reinforcement Learning, en el Aprendizaje Supervizado los modelos _estudian_ estas series de datos ya conocidos para lograr abstraer y reconocer las características útiles para llevar a cabo la tarea objetivo. Todo esto por supuesto, de manera automática.\r\n",
        "\r\n",
        "Para familiarizarnos con la implementación de estos modelos, en este workshop estudiaremos un caso sencillo de clasificación inspirado en el Playground de TensorFlow. En este caso, los puntos son segmentados o bien, etiquetados, en dos regiones dentro de los cuatro cuadrantes que se muestran en el diagrama a continuación. Así, el objetivo de nuestro modelo será clasificar la etiqueta de los puntos dentro de este espacio apartir del par de puntos $(x1, x2)$ como valores de entrada.\r\n",
        "\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_06/bin/problem_ref.png\" height=\"280\">\r\n",
        "\r\n",
        "Para lograr esto, actualmente existen varios frameworks para desarrollar redes neuronales de Aprendizaje Profundo. No obstante, ahora nos concentraremos en la construcción e implementación de modelos de clasificación de deep learning mediante `TensorFlow`.\r\n",
        "\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/tensorflow.png\" width=\"400\">\r\n",
        "\r\n",
        "`TensorFlow`, en términos generales, consiste en un framework diseñado para desarrollar e implementar algoritmos de Machine Learnine, y por supuesto, entre ellos, modelos de Deep Learning. Una de las particulares de este framework es que ofrece toda una gama de niveles de abstracción, desde el desarrollo de modelos de mayor complejidad mediante herramientas `low-level`, hasta la compilación y el entrenamiento de arquitecturas mediante estructuras `high-level`, como la API Keras.\r\n",
        "\r\n",
        "Puede encontrar la documentación de estas librerías en los siguientes links:\r\n",
        "- https://www.tensorflow.org/api_docs/python/\r\n",
        "- https://keras.io/api/\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKPEa_dadrYt",
        "outputId": "c7884ddc-cce1-4769-86e5-9e84e6220b90"
      },
      "source": [
        "!git clone https://github.com/cherrerab/roboticafcfm.git\r\n",
        "%cd /content/roboticafcfm"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'roboticafcfm'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 409 (delta 13), reused 0 (delta 0), pack-reused 362\u001b[K\n",
            "Receiving objects: 100% (409/409), 14.27 MiB | 7.34 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n",
            "/content/roboticafcfm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgd28LK18v5m"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "# dataset size (cantidad de puntos)\r\n",
        "N = 2000\r\n",
        "\r\n",
        "# ---\r\n",
        "# inicializar datasets\r\n",
        "X = np.zeros( (N, 2) )\r\n",
        "Y = np.zeros( (N, 2) )\r\n",
        "\r\n",
        "for i in range(N):\r\n",
        "\r\n",
        "  # generar datos (x1, x2) de manera random dentro de [0, 1]\r\n",
        "  x1 =\r\n",
        "  x2 =\r\n",
        "\r\n",
        "  # guardar en el set X\r\n",
        "  X[i, 0] =\r\n",
        "  X[i, 1] =\r\n",
        "\r\n",
        "  # obtener etiqueta del punto (x1, x2)\r\n",
        "  y = (x1 - 0.5)*(x2 - 0.5)\r\n",
        "\r\n",
        "  # etiqueta amarilla [0, 1]\r\n",
        "  if y >= 0:\r\n",
        "    Y[i, 0] =\r\n",
        "    Y[i, 1] =\r\n",
        "\r\n",
        "  # etiqueta azul [1, 0]\r\n",
        "  else:\r\n",
        "    Y[i, 0] =\r\n",
        "    Y[i, 1] =\r\n",
        "\r\n",
        "# ---\r\n",
        "# print muestra del dataset\r\n",
        "print('punto número: 100')\r\n",
        "print('X ', X[100, :])\r\n",
        "print('Y ', Y[100, :])\r\n",
        "\r\n",
        "# ---\r\n",
        "# visualizar datos en el plano\r\n",
        "fig = plt.figure( figsize=(7, 7) )\r\n",
        "plt.scatter(X[:,0], X[:,1], c=Y[:,1], alpha=0.5, s=60, cmap='plasma')\r\n",
        "plt.scatter(X[100, 0], X[100, 1], c='r', marker='*', s=60)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf092COWFhR-"
      },
      "source": [
        "## Data Splitting\r\n",
        "\r\n",
        "Teniendo ya el dataset para el entrenamiento de la red neuronal, se debe dividir este en dos sets: uno de entrenamiento (`training set`) y otro de testing (`testing set`). El primero es utilizado, como su nombre lo indica, en el entrenamiento de la red neuronal mientras que el segundo es utilizado para evaluar el desempeño del modelo ya entrenado. La metrica utilizada comúnmente para evaluar un clasificador es el `accuracy`, el cual cuantifica la cantidad de aciertos de clasificación respecto al total de muestras, en un determinado conjunto de evaluación.\r\n",
        "\r\n",
        "De este modo, como la red es evaluada sobre datos que \"nunca ha visto\" durante el entrenamiento, el `accuracy` permite observar si la red ha logrado generalizar el problema o si se ha _overfitteado_ (aprenderse los datos de entrenamiento de \"memoria\").\r\n",
        "\r\n",
        "El _data splitting_ se puede lograr con el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOVnSiaaFg_2"
      },
      "source": [
        "# importar librerías\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# ---\r\n",
        "# generar sets de datos de training y testing\r\n",
        "# la varibale test_size permite controlar la proporción entre los datos de testing y training.\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = )\r\n",
        "\r\n",
        "# adicionalmente generaremos un conjunto de validación\r\n",
        "# este conjunto será utilizado para monitorear la generalización del modelo\r\n",
        "# sin utilizar el conjunto de testing.\r\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = )\r\n",
        "\r\n",
        "# ---\r\n",
        "# visualizar\r\n",
        "plt.figure( figsize=(7, 7) )\r\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c='g', alpha=0.5, s=60, label='train')\r\n",
        "plt.scatter(X_val[:,0], X_val[:,1], c='b', alpha=0.5, s=60, label='val')\r\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c='r', alpha=0.5, s=60, label='test')\r\n",
        "plt.grid()\r\n",
        "plt.legend()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efN7j8eS-CP"
      },
      "source": [
        "# Model Building\r\n",
        "\r\n",
        "Con los sets de entrenamiento y testing listos se puede dar paso a la configuración y entrenamiento de nuestra red neuronal. Para esto se utilizará principalmente la librería `keras` o `tf.keras`. Keras es una API de alto nivel para la creación y el entrenamiento de modelos de deep learning. Está orientada y diseñada para la construcción de modelos de forma modular o en bloques. De este modo, ofrece un framework mucho más amigable e intuitivo para principiantes, a la vez que mantiene un estructura personalizable y versátil que permite a usuarios más avanzados incorporar nuevas ideas.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/keras_logo.png\" width=\"400\">\r\n",
        "\r\n",
        "\r\n",
        "En este caso, se configurará una red Dense que consiste básicamente en capas de nodos que se van ordenando a medida que se agregan a la red.\r\n",
        "\r\n",
        "La primera capa debe corresponderse con las variables de inputs con las que se alimenta a la red y por tanto ha de tener dos nodos, uno para cada variable x1 y x2.\r\n",
        "\r\n",
        "Por otro lado, la última capa ha de tener dos nodos cuyo valor (output) debe corresponder a la etiqueta del punto (x1, x2). Como este valor solo puede ser 1 o 0, la función de activación de este nodo debe ser softmax (similar a la función de escalón unitario).\r\n",
        "\r\n",
        "A continuación se presenta el código necesario para configurar una red simple de este tipo (Dense)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Cv6hDLTjEc"
      },
      "source": [
        "## Model Setup\r\n",
        "Los elemenos básicos para la construcción de un modelo o `keras.Model` consisten en las capas o `layers` del modelo. En este sentido, configurar un modelo en Keras resulta en ir uniendo o conectando capas `keras.layers` de manera secuencial.\r\n",
        "\r\n",
        "Para comenzar e introducir el framework de esta librería, construiremos un modelo o red neuronal `Sequential` a partir de únicamente capas `keras.layers.Dense` y otras capas elementales.\r\n",
        "- https://keras.io/api/layers/\r\n",
        "- https://keras.io/api/layers/activations/\r\n",
        "\r\n",
        "De esta forma, en términos generales, compondremos nuestro modelo de una serie de capas `Dense`, que se encargarán de procesar la información y los patrones de los datos de entrada hasta una última capa `Dense` con únicamente 2 nodos que determinarán las etiquetas de los puntos $(x_1, x_2)$ que se ingresen al modelo.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/roboticafcfm/master/auxiliar_06/bin/model_ref.png\" height=\"200\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLeVpqC5S90Y"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "\r\n",
        "# inicializar modelo keras.Sequential\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# ahora podemos ir agregando secuencialmente capas nuestro modelo\r\n",
        "# mediante el método keras.Model.add\r\n",
        "\r\n",
        "# ---\r\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\r\n",
        "# las dimensiones de los datos que se ingresarán al modelo\r\n",
        "input_dim = ( , )\r\n",
        "model.add( Input( shape=input_dim ) )\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora debemos ir agregando nuestras capas Dense.\r\n",
        "# https://keras.io/api/layers/core_layers/dense/\r\n",
        "\r\n",
        "# las keras.layers.Dense reciben la cantidad de nodos o units dentro\r\n",
        "# de la capa y la función de activación que operarán.\r\n",
        "# https://keras.io/api/layers/activations/\r\n",
        "model.add( Dense( units=, activation= ) )\r\n",
        "model.add(  )\r\n",
        "\r\n",
        "# ---\r\n",
        "# por último debemos configurar nuestra capa de salida\r\n",
        "# dado que el modelo consiste en uno de clasificación emplearemos\r\n",
        "# la función softmax, donde cada nodo indicará la probabilidad de que\r\n",
        "# los datos correspondan a una de las etiquetas o estados de salud.\r\n",
        "labels_num = \r\n",
        "model.add( Dense(units=labels_num, activation=) )"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYiIqdxTXFiV"
      },
      "source": [
        "Así, hemos construido nuestro modelo de clasificación el cual consiste de una capa de entrada `Input`, tres capas ocultas Fully Connected o `Dense` con 64 nodos de activación ReLU cada una, y una capa de salida `Dense` de 2 nodos con función de activación `softmax`. De esta manera, cada uno de los nodos de salida estará asociado a una de las etiquetas en el problema.\r\n",
        "\r\n",
        "Para imprimir información sobre el modelo generado, Keras cuenta con el método `keras.Model.summary` para presentar un resumen de la arquitectura de la red neuronal.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTqwjlmrXlYf"
      },
      "source": [
        "# imprimir object o clase del modelo\r\n",
        "print('type(model): ', type(model))\r\n",
        "\r\n",
        "# imprimir resumen del modelo\r\n",
        "print('\\nclassification model summary:\\n')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5pY1rU0ZCi-"
      },
      "source": [
        "## Compile Model\r\n",
        "Antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\r\n",
        "- https://keras.io/api/models/model_training_apis/\r\n",
        "- https://keras.io/api/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0dHeFUiZhtt"
      },
      "source": [
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "# configurar optimizador Adam\r\n",
        "# https://keras.io/api/optimizers/adam/\r\n",
        "opt = \r\n",
        "\r\n",
        "# ---\r\n",
        "# compilar modelo siguiendo como función de pérdida\r\n",
        "# la categorical crossentropy, 'categorical_crossentropy'\r\n",
        "model.compile(loss=, optimizer=, metrics=[])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMjKnjadZu60"
      },
      "source": [
        "## Model Training\r\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros.\r\n",
        "\r\n",
        "Para entrenar `keras.Models` se utiliza el método `keras.Model.fit`, el cual aparte de iniciar y realizar la rutina de entrenamiento, retorna un registro `History`. Mediante `History.history` es posible acceder a la evolución de la función de pérdida durante el entrenamiento tanto sobre los datos de `train` como sobre los de `validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k_XumF4Z5CM"
      },
      "source": [
        "from utils import plot_loss_function\r\n",
        "\r\n",
        "# realizar rutina de entrenamiento\r\n",
        "train_history = model.fit(X_train, Y_train,\r\n",
        "                          batch_size=, epochs=,\r\n",
        "                          validation_data= )\r\n",
        "\r\n",
        "# plot gráfico de función de pérdida\r\n",
        "plot_loss_function(train_history, figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03WTk-abf-vY"
      },
      "source": [
        "## Model Evaluation\r\n",
        "Finalmente, una vez entrenado nuestro modelo debemos evaluar su desempeño. En este caso particular, debemos usar los datos que aislamos para `testing` `(X_test, Y_test)`. Para utilizar el `keras.Model` sobre nuevos datos de clasificación, conviene utilizar el método `keras.Sequential.predict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9lSrNCPd_DI"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "from utils import plot_classification_map\r\n",
        "from utils import plot_confusion_matrix\r\n",
        "\r\n",
        "# obtener predicciones de X_test con model.predict\r\n",
        "Y_pred = \r\n",
        "\r\n",
        "# para que el resultado nos sea más intuitivo transformaremos\r\n",
        "# las etiquetas nuevamente a non one-hot-encoding\r\n",
        "# utilizando np.argmax\r\n",
        "Y_pred = \r\n",
        "Y_true = \r\n",
        "\r\n",
        "# calcular accuracy de la clasificación.\r\n",
        "accuracy = accuracy_score(Y_true, Y_pred)\r\n",
        "print('testing accuracy: {:1.3f}'.format(accuracy))\r\n",
        "\r\n",
        "# ---\r\n",
        "# visualizar evaluación\r\n",
        "\r\n",
        "# matriz de confusión\r\n",
        "plot_confusion_matrix(Y_true, Y_pred,\r\n",
        "                      target_names=['0', '1'], figsize=(6, 6))\r\n",
        "\r\n",
        "# classification map\r\n",
        "plot_classification_map(model, (0,1), (0,1), 256)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}